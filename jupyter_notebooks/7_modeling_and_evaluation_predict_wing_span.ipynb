{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a regression model to predict Wing Span for Single piston Engined airplanes.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/airplane_performance_study.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* feature_importance (3 files) \n",
        "* Mean Squared Error\n",
        "* Predicted vs Actual Plot\n",
        "* Residuals Distribution Plot\n",
        "* Residuals vs Fitted Plot\n",
        "* Train set\n",
        "* Test set\n",
        "* Modeling pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* Access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to not compare apples with oranges the code below filters out all Multi Engined, jet and propjet leaving only the subset of **single Piston Engines**. It drops these columns as well as the performance modification (TP mods) column and the Hmax_(One) and ROC_(One)-columns since they only pertain to Multi Engined airplane."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\")\n",
        "      .query(\"Engine_Type == 0 and Multi_Engine == 0 and TP_mods == 0\")  # subset of airplanes with single Piston Enginees without any Turbo Prop performance modification TP_mods dmultiple Engines\n",
        "      .drop(labels=['Model', 'Company', 'THR', 'SHP', 'Engine_Type', 'Multi_Engine', 'TP_mods', 'Hmax_(One)', 'ROC_(One)'], axis=1)\n",
        "      )\n",
        "\n",
        "print(df.shape)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define features and target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define features and target\n",
        "\n",
        "X are the features for the model (all columns in df except for target Wing_Span.\n",
        "y is the target variable which is the Wing_Span column in the df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Wing_Span', axis=1)  # Features (where I drop the target variable)\n",
        "y = df['Wing_Span']                # Target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Train Test Set\n",
        "\n",
        "80% of df is split into the train set and 20% is left for the test set.\n",
        "The random_state=0 parameter ensures reproducibility of the split.\n",
        "\n",
        "We see that we have a suffiecent numper of data points to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  # 80% to train and 20% to test.\n",
        "\n",
        "print(\"* Train set:\", X_train.shape, y_train.shape,\n",
        "      \"\\n* Test set:\",  X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MP Pipeline: Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Pipeline\n",
        "\n",
        "'scaler' applies standard scaling to the (numerical) features, ensuring that they have a mean = 0 and a standard deviation = 1.\n",
        "'regressor' initializes Random Forest regression model for predicting target variable (Wing_Span)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),                     # Scale numerical features\n",
        "    ('regressor', RandomForestRegressor(random_state=0))  # Regression model using Random Forrest\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the model\n",
        "\n",
        "build the training model using the training dataset.\n",
        "\n",
        "StandardScaler is applied to scale the features in X_train.\n",
        "\n",
        "RandomForestRegressor is trained on the scaled features (from previous step) and corresponding target values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make predictions\n",
        "\n",
        "Scaling test features using StandardScaler fitted during training.\n",
        "\n",
        "Using the trained RandomForestRegressor to predict on scaled test data.\n",
        "\n",
        "Output: The predicted values for target (Wing_Span) stored in the predictions variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Squared Error Calculation\n",
        "\n",
        "The mean_squared_error computes the mean squared error (MSE) between the actual values (y_test) and the predicted values (predictions). The MSE provids a quantitative measure of the model's performance on the test dataset.\n",
        "\n",
        " MSE is ok but not great, typically off by almost 3 ft (square root on MSE)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(f'Mean Squared Error: {mse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save above output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save MSE to a text file\n",
        "with open('outputs/ml_pipeline/predict_analysis/mse.txt', 'w') as f:\n",
        "    f.write(f'Mean Squared Error: {mse}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Print feature importances (for RandomForestRegressor)\n",
        "\n",
        "feature_importance_df is created that in descending order (by importance) list an \"importance score\"-for each feature providing insights into which features are most influential in predicting the target variable (Wing_Span).It makes much sense that the weights are on top since the Wing Span is related to the Wing Area (not in the data set) and that Wing Area is directly related to lift that in turn is directly related to the weights (since the lift need to offset the weight in order for the airplane to fly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importances = pipeline.named_steps['regressor'].feature_importances_\n",
        "feature_names = X.columns\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save above output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the feature importances as a .pkl file\n",
        "joblib.dump(importances, 'outputs/ml_pipeline/predict_analysis/feature_importances.pkl')\n",
        "\n",
        "# Save the feature names as a .pkl file\n",
        "joblib.dump(feature_names.tolist(), 'outputs/ml_pipeline/predict_analysis/feature_names.pkl')\n",
        "\n",
        "# Save the feature importance DataFrame as a .pkl file\n",
        "joblib.dump(feature_importance_df, 'outputs/ml_pipeline/predict_analysis/feature_importance_df.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicted vs actual values\n",
        "The points in the scatter plot represent the predicted Wing_Span values (on the y-axis) against the actual Wing_Span values (on the x-axis) for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, predictions, alpha=0.6)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Diagonal line\n",
        "plt.title('Predicted vs Actual Wing Span')\n",
        "plt.xlabel('Actual Wing Span')\n",
        "plt.ylabel('Predicted Wing Span')\n",
        "plt.xlim([y_test.min(), y_test.max()])\n",
        "plt.ylim([y_test.min(), y_test.max()])\n",
        "plt.grid()\n",
        "plt.savefig('outputs/ml_pipeline/predict_analysis/predicted_vs_actual.png')  # Save the plot as a PNG file\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residual plot\n",
        "\n",
        "Calculating Residuals: The residuals are calculated by subtracting the predicted values (predictions) from the actual values (y_test). Residuals indicate the difference between what the model predicted and what the actual values were.\n",
        "\n",
        "This histogram helps visualize the distribution of the residuals, which can indicate how well the model is performing. Ideally, residuals should be normally distributed around zero if the model is a good fit.\n",
        "\n",
        "The Residuals Distribution shows that the distribution of errors (if negelecting the outlier) are such that all errors are all well within +- 5 meter. (and that the model rather tend overestimate than underestimate the wingspan)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residuals = y_test - predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title('Residuals Distribution')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid()\n",
        "plt.savefig('outputs/ml_pipeline/predict_analysis/residuals_distribution.png')  # Save the plot as a PNG file\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residuals vs Fitted plot\n",
        "\n",
        "Each point represents a fitted value (predicted wingspan) from the train set plotted against its corresponding residual.\n",
        "\n",
        "Ideally, the residuals should be randomly scattered around the horizontal line at zero without any discernible pattern however our pattern appear to be \"high\" which is consistent with our previous plots.\n",
        "\n",
        "If we the \"High pattern\" is real it could indicate: it may indicate issues such as non-linearity, omitted variables, or model miss-specification.\n",
        "\n",
        "We also notice an outlier at the bottom of the graf that we could hunt down and remove since it potentially could skew the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(predictions, residuals, alpha=0.6)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Fitted')\n",
        "plt.xlabel('Fitted Values (Predicted Wing Span)')\n",
        "plt.ylabel('Residuals')\n",
        "plt.grid()\n",
        "plt.savefig('outputs/ml_pipeline/predict_analysis/residuals_vs_fitted.png')  # Save the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A second Evaluation of model based on observation in plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All previous plots have appearntly given a slight overestimation of the Wingspan. This is confirmed by a \n",
        "Mean Error (ME) of approximately 5%. The Relative Error of 0.18% (based on a Wingspan range for the y_test of 29.08 feet) shows that this is error is not significant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_error = np.mean(predictions - y_test)\n",
        "print(\"Mean Error (ME):\", mean_error)\n",
        "\n",
        "range_value = y_test.max() - y_test.min()\n",
        "print(f\"Range of y_test: {range_value:.2f} feet\")\n",
        "\n",
        "# Calculate the relative error\n",
        "relative_error = mean_error / range_value\n",
        "print(f\"Relative Error: {relative_error:.4f} ({relative_error * 100:.2f}%)\")\n",
        "\n",
        "# Save the results to a text file\n",
        "output_path = 'outputs/ml_pipeline/predict_analysis/error_analysis.txt'\n",
        "with open(output_path, 'w') as f:\n",
        "    f.write(f'Mean Error (ME): {mean_error:.4f} feet\\n')\n",
        "    f.write(f'Range of y_test: {range_value:.2f} feet\\n')\n",
        "    f.write(f'Relative Error: {relative_error:.4f} ({relative_error * 100:.2f}%)\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push files to the repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShuJ5tYUC06o"
      },
      "source": [
        "The following plots has already been save upstream right after they where created:\n",
        "* feature_importance (3 files) \n",
        "* Mean Squared Error\n",
        "* Predicted vs Actual Plot\n",
        "* Residuals Distribution Plot\n",
        "* Residuals vs Fitted Plot\n",
        "\n",
        "Below we will generate the following files:\n",
        "* Train set\n",
        "* Test set\n",
        "* Modeling pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(pipeline, 'outputs/ml_pipeline/predict_analysis/wingspan_predictor_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vBpPvnaG5Mb"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "file_path = f'outputs/ml_pipeline/predict_analysis'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TvoMsi3DNw1"
      },
      "source": [
        "## Train Set: features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJHmwyqgDOr1"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.head().to_csv('outputs/ml_pipeline/predict_analysis/X_train_head.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB6pjmAcDOym"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train.to_csv('outputs/ml_pipeline/predict_analysis/y_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVbS3OnRDYtJ"
      },
      "source": [
        "## Test Set: features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbgF38n1DaPp"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test.head().to_csv('outputs/ml_pipeline/predict_analysis/X_test_head.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jz66iMaDacI"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test.to_csv('outputs/ml_pipeline/predict_analysis/y_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modeling and Evaluation - Predict Tenure.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
