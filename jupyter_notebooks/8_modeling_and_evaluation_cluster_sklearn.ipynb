{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a cluster model to group similar data\n",
        "* Understand the profile for each cluster\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/airplane_performance_study.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Most important features to define a cluster plot\n",
        "* Clusters Profile Description\n",
        "* Cluster Silhouette\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* Access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make the parent of the parent of current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are dropping:\n",
        "* Model, Company because these are only meta data/identifiers\n",
        "* 'Multi_Engine' and 'Engine_Type' becasue we want to evaluate against these\n",
        "* THR and SHP both because they are difficult to interpret\n",
        "* Hmax_(One) and ROC_(One) becasue since they ONLY apply to the Airplanes with a \"True\" under the \"Multi Engine\"-feature it would therefore skew the result. A run that included Hmax_(One) and ROC_(One) indeed show Hmax_(One) on top as the most important feature however ROC_(One) was even listed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\")\n",
        "      .drop(['Model', 'Company', 'Multi_Engine', 'Engine_Type', 'THR', 'SHP', 'Hmax_(One)', 'ROC_(One)'], axis=1)\n",
        "      )\n",
        "print(df.shape)\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Cluster Pipeline with all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "##  ML Cluster Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert boolean columns to strings\n",
        "df['TP_mods'] = df['TP_mods'].replace({True: 'Yes', False: 'No'})\n",
        "\n",
        "# Convert integer columns to strings (if they represent categories)\n",
        "#df['Engine_Type'] = df['Engine_Type'].astype(str)\n",
        "\n",
        "# Check the updated data types\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use K-means since this model is ideal for numerical continuous data and the majority of our data is numerical continuous. However it is mixed (both categorical and numeric continuous) which could warrant another method like K-modes or K-prototypes that are better suited for mixed data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Threshold is set to 0.5 which is a very low (values are typically 0.85 - 0.9) which is first of all since our values are relatively normally distributed and we shall see that even this low threshold retains only two features! We have experimented with many different threshold values without seeing any significantly difference on n_components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Feature Engineering\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "\n",
        "# Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ML algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "def PipelineCluster():\n",
        "    \"\"\"\n",
        "    Creates a clustering pipeline for feature engineering, scaling, and dimensionality reduction.\n",
        "\n",
        "    This function constructs a machine learning pipeline that performs the following steps:\n",
        "    1. Applies an Ordinal Encoder to specified categorical variables (`TP_mods` and `Engine_Type`) \n",
        "       using an arbitrary encoding method.\n",
        "    2. Utilizes Smart Correlated Selection to filter out features based on correlation, \n",
        "       removing those that are highly correlated with a threshold of 0.5.\n",
        "    3. Scales the features using StandardScaler to standardize the dataset to have a mean of 0 \n",
        "       and a standard deviation of 1.\n",
        "    4. Performs Principal Component Analysis (PCA) to reduce the dimensionality of the dataset \n",
        "       to 50 components, aiding in visualization and reducing noise.\n",
        "    5. Applies KMeans clustering to identify 50 clusters in the transformed feature space.\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: A scikit-learn Pipeline object that can be used for fitting and transforming\n",
        "        data prior to clustering.\n",
        "\n",
        "    Categorical variables:\n",
        "        - TP_mods\n",
        "        - Engine_Type\n",
        "\n",
        "    KMeans parameters:\n",
        "        - n_clusters: 50\n",
        "        - random_state: 0 (for reproducibility)\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['TP_mods'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.5, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"PCA\", PCA(n_components=50, random_state=0)),\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrr31sD9DyvY"
      },
      "source": [
        "## Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We only recieve 2 components. Since the components is 0% and 100% the elbow method (select a number of components around a variance of 80%.) is rendered useless. Other dimensionality reduction techniques like t-SNE or UMAP could be considered.\n",
        "\n",
        "The \"SmartCorrelatedSelection\" reduces the number of rows (dimensionality reduction) depending on what threshold value we set! https://feature-engine.trainindata.com/en/latest/user_guide/selection/SmartCorrelatedSelection.html\n",
        "\n",
        "We are not concerned with the warnings below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es49S65qqvRw"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "\n",
        "print(df_pca.shape,'\\n', type(df_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlABEj9Iw6Jr"
      },
      "source": [
        "Apply PCA separately to the scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_pca[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM_Xsqxsrt5M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "n_components = 2 # Adapted this to two since this value cannot be higher than \n",
        "# the number of retained columns in the df_pca which was only three with a \n",
        "# threshold value as low as 0,6\n",
        "\n",
        "\n",
        "def pca_components_analysis(df_pca, n_components):\n",
        "    \"\"\"\n",
        "    Performs Principal Component Analysis (PCA) on the provided dataset and visualizes the explained variance.\n",
        "\n",
        "    This function fits a PCA model to the input DataFrame, transforms the data, \n",
        "    and calculates the explained variance ratio for each principal component. \n",
        "    It then plots the explained variance ratio and accumulated variance as a line plot.\n",
        "\n",
        "    Parameters:\n",
        "        df_pca (DataFrame): A pandas DataFrame containing the standardized data to be analyzed by PCA.\n",
        "        n_components (int): The number of principal components to retain, which should be less than or \n",
        "                            equal to the number of features in `df_pca`.\n",
        "\n",
        "    Outputs:\n",
        "        Prints the total percentage of variance explained by the specified number of components \n",
        "        and displays a line plot showing the explained variance ratio and accumulated variance.\n",
        "\n",
        "    Notes:\n",
        "        - Ensure that `df_pca` is preprocessed and suitable for PCA (e.g., scaled if necessary).\n",
        "        - The number of components must be chosen based on the retained columns in `df_pca`.\n",
        "    \"\"\"\n",
        "    pca = PCA(n_components=n_components).fit(df_pca)\n",
        "    x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
        "\n",
        "    ComponentsList = [\"Component \" + str(number)\n",
        "                      for number in range(n_components)]\n",
        "    dfExplVarRatio = pd.DataFrame(\n",
        "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
        "        index=ComponentsList,\n",
        "        columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
        "    )\n",
        "\n",
        "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    sns.lineplot(data=dfExplVarRatio,  marker=\"o\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(np.arange(0, 110, 10))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "pca_components_analysis(df_pca=df_pca, n_components=n_components)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_components_analysis(df_pca=df_pca,n_components=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Plot with Accumulated CVariance Ratio incldue only 2 components (n_components=2) and we select to retain both thesse since our main purpos with this \n",
        "analysis is interpretability and we want to retain as much information as possible for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    \"\"\"\n",
        "    Please see doc string for the first appearance of the function \"PipelineCluster\" higher up \n",
        "    in this notebook. Note that the input varies between the different instances of this function.\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['TP_mods'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.5, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        # we update n_components to 2\n",
        "        (\"PCA\", PCA(n_components=2, random_state=0)),\n",
        "\n",
        "        (\"model\", KMeans(n_clusters=50, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw9NtDj4EtEJ"
      },
      "source": [
        "## Elbow Method and Silhouette Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVaMnb9vGyBw"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df)\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZBcHjt7EwFT"
      },
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11)) # 11 is not inclusive, it will plot until 10\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "# 6 is not inclusive, it will stop at 5\n",
        "n_cluster_start, n_cluster_stop = 2, 6\n",
        "\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
        "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
        "\n",
        "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
        "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                      colors='yellowbrick')\n",
        "    visualizer.fit(df_analysis)\n",
        "    visualizer.show()\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Elbow Method gives a cluster size of two or three. The Silhouette shadow plots appear to clearly point towards 2 clusters so this will be our choice. With two clusters both reaches past the mean line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "    \"\"\"\n",
        "    Please see doc string for the first appearance of the function \"PipelineCluster\" higher up \n",
        "    in this notebook. Note that the input varies between the different instances of this function.\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['TP_mods'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.5, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"PCA\", PCA(n_components=2, random_state=0)),\n",
        "\n",
        "        # we update n_clusters to 2\n",
        "        (\"model\", KMeans(n_clusters=2, random_state=0)),\n",
        "\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineCluster()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "## Fit Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxaylKk-6CQ"
      },
      "source": [
        "Quick recap of our data for training cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfKHc63v-6Zm"
      },
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L0iMkjJHXSI"
      },
      "source": [
        "## Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT5IjmTmei8"
      },
      "source": [
        "We add a column \"Clusters\" (with the cluster pipeline predictions) to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow8B0xVdmlgK"
      },
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAVrYJEqxYyG"
      },
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_analysis.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1],  # Original code is: y=df_analysis[:, 1] but that gives an error since it tries to acces the second column of a one column df\n",
        "                hue=X['Clusters'], palette='Set1', alpha=0.6)\n",
        "plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 1], y=pipeline_cluster['model'].cluster_centers_[:, 1],  # Same here! Original code: cluster_centers_[:, 0]\n",
        "            marker=\"x\", s=169, linewidths=3, color=\"black\")\n",
        "plt.xlabel(\"PCA Component 0\")\n",
        "plt.ylabel(\"PCA Component 1\")\n",
        "plt.title(\"PCA Components colored by Clusters\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fact that the centroid for the smaller cluster (blue) is far away (lateraly) from the actual datapoint (but close longitudinally) might suggest low intrinsic dimensionality that the n_components should have been choosen to 1 instead of 2. Other reasons could be: Poor Cluster Quality, Outliers, High Variability or Feature Scaling Issues.\n",
        "\n",
        "The alignment of clusters along lines indicates that there are strong linear relationships in the data and therefore perhaps the cluster patterns could be captured with fewer components. The linear relationships probably has the background that the performance features follow similar patterns (what is good for one feature is also good for another feature).The fact that the clusters are clearly spaced from each other, like the smallest (green) cluster suggest that this is a categorical feature that \"lifts\" all the other features an equal part. Such a candidate would me the TP mods that we have seen stick out before. This is also one of only two categorical values since we have dropped Multi Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWgb0kPOWtMa"
      },
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables = X['Clusters']\n",
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWTf1rgkQ7b"
      },
      "source": [
        "## Fit a classifier, where the target is cluster predictions and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP6sGUn0XyDm"
      },
      "source": [
        "We copy `X` to a DataFrame `df_clf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeLq81sm2yAg"
      },
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3Ei6Os5X3s"
      },
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgHXehCVyzUl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_clf.drop(['Clusters'], axis=1),\n",
        "    df_clf['Clusters'],\n",
        "    test_size=0.2,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZUk-uV5aN8"
      },
      "source": [
        "Create classifier pipeline steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML algorithm\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "def PipelineClf2ExplainClusters():\n",
        "    \"\"\"\n",
        "    Constructs a machine learning pipeline for classification tasks with feature selection and scaling.\n",
        "\n",
        "    The pipeline includes the following steps:\n",
        "        1. **Ordinal Encoding**: Transforms categorical variables into ordinal format using\n",
        "           an arbitrary encoding method for the variable 'TP_mods'.\n",
        "        2. **Smart Correlation Selection**: Selects features based on their correlation to the target\n",
        "           variable using the Spearman method, with a threshold of 0.5 and a variance-based selection method.\n",
        "        3. **Standard Scaling**: Standardizes the features by removing the mean and scaling to unit variance.\n",
        "        4. **Feature Selection**: Applies SelectFromModel using a Gradient Boosting Classifier to select\n",
        "           the most important features.\n",
        "        5. **Model Training**: Finally, a Gradient Boosting Classifier is used as the main model for training.\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: A scikit-learn Pipeline object configured with the specified preprocessing and model\n",
        "        training steps.\n",
        "    \"\"\"\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['TP_mods'])),\n",
        "\n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\",\n",
        "                                                              threshold=0.5, selection_method=\"variance\")),\n",
        "\n",
        "        (\"scaler\", StandardScaler()),\n",
        "\n",
        "        (\"feat_selection\", SelectFromModel(\n",
        "            GradientBoostingClassifier(random_state=0))),\n",
        "\n",
        "        (\"model\", GradientBoostingClassifier(random_state=0)),\n",
        "\n",
        "    ])\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineClf2ExplainClusters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the classifier to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R7xdg1Av0Ce"
      },
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z05LMFoZ4T2K"
      },
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1iqL2Kc544K"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oo4xJMZ615p"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwjHBSh5ejG"
      },
      "source": [
        "## Assess the most important Features that define a cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most important feature being TP mods is rather disappointing since this is a trivial result from an Aircraft Design point of view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# after data cleaning and feature engineering, the feature space changes\n",
        "\n",
        "# how many data cleaning and feature engineering steps does your pipeline have?\n",
        "data_cleaning_feat_eng_steps = 2\n",
        "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support(\n",
        ")].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# reassign best features in importance order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgul0EF9nx_E"
      },
      "source": [
        "We will store the best_features to use at a later stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzyMkwHznyG8"
      },
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables = best_features\n",
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ywCxJmkRQn"
      },
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMr-wiudEkb"
      },
      "source": [
        "Load function that plots a table with description for all Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_lpRVDqTdEul"
      },
      "outputs": [],
      "source": [
        "\n",
        "def DescriptionAllClusters(df, decimal_points=3):\n",
        "    \"\"\"\n",
        "    Generates a summary description for each cluster in the provided DataFrame.\n",
        "\n",
        "    This function iterates through each unique cluster in the input DataFrame, \n",
        "    subsets the data for each cluster, and computes a detailed description using \n",
        "    the `Clusters_IndividualDescription` function. The results are compiled into \n",
        "    a new DataFrame, which is returned.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): A pandas DataFrame containing data with a 'Clusters' column \n",
        "                        that indicates the cluster assignment for each row.\n",
        "        decimal_points (int, optional): The number of decimal points to display in \n",
        "                                         the summary statistics. Default is 3.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A DataFrame containing the summary description of each cluster, \n",
        "                    indexed by cluster.\n",
        "\n",
        "    Notes:\n",
        "        - The input DataFrame must include a column named 'Clusters' to categorize \n",
        "          the data into distinct groups.\n",
        "        - The function assumes that the `Clusters_IndividualDescription` function \n",
        "          is defined elsewhere and handles the individual cluster description logic.\n",
        "    \"\"\"\n",
        "    DescriptionAllClusters = pd.DataFrame(\n",
        "        columns=df.drop(['Clusters'], axis=1).columns)\n",
        "    # iterate on each cluster , calls Clusters_IndividualDescription()\n",
        "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
        "\n",
        "        EDA_ClusterSubset = df.query(\n",
        "            f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
        "        ClusterDescription = Clusters_IndividualDescription(\n",
        "            EDA_ClusterSubset, cluster, decimal_points)\n",
        "        DescriptionAllClusters = DescriptionAllClusters.append(\n",
        "            ClusterDescription)\n",
        "\n",
        "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
        "    return DescriptionAllClusters\n",
        "\n",
        "\n",
        "def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\n",
        "    \"\"\"\n",
        "    Generates a descriptive summary for a specified cluster in a DataFrame, including statistics for numerical \n",
        "    and categorical variables.\n",
        "\n",
        "    For each column in the input DataFrame (`EDA_Cluster`):\n",
        "    - If the column is categorical, the function counts the frequencies of the top three most common categories \n",
        "      and calculates their respective percentages.\n",
        "    - If the column is numerical, it calculates the interquartile range (IQR) using the first (Q1) and third \n",
        "      quartiles (Q3), providing a range that captures the most common values.\n",
        "\n",
        "    Args:\n",
        "        EDA_Cluster (pd.DataFrame): A DataFrame containing data to analyze, where each row corresponds to an \n",
        "                                     instance and each column corresponds to a variable.\n",
        "        cluster (int or str): An identifier for the specific cluster being described.\n",
        "        decimal_points (int): The number of decimal places to round the quartile values.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing a summary description for the specified cluster, with columns \n",
        "                      representing the original variables and a final column indicating the cluster identifier.\n",
        "\n",
        "    Notes:\n",
        "        - If a variable has only missing data within the specified cluster, the function will report \n",
        "          'Not available' for that variable.\n",
        "        - Any errors encountered during processing will be printed to the console, along with details about \n",
        "          the specific variable and cluster.\n",
        "    \"\"\"\n",
        "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "    # for a given cluster, iterate over all columns\n",
        "    # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
        "    # That will show the range for the most common values for the numerical variable\n",
        "    # if the variable is categorical, count the frequencies and displays the top 3 most frequent\n",
        "    # That will show the most common levels for the category\n",
        "\n",
        "    for col in EDA_Cluster.columns:\n",
        "\n",
        "        try:  # eventually a given cluster will have only missing data for a given variable\n",
        "\n",
        "            if EDA_Cluster[col].dtypes == 'object':\n",
        "\n",
        "                top_frequencies = EDA_Cluster.dropna(\n",
        "                    subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "                Description = ''\n",
        "\n",
        "                for x in range(len(top_frequencies)):\n",
        "                    freq = top_frequencies.iloc[x]\n",
        "                    category = top_frequencies.index[x][0]\n",
        "                    CategoryPercentage = int(round(freq*100, 0))\n",
        "                    statement = f\"'{category}': {CategoryPercentage}% , \"\n",
        "                    Description = Description + statement\n",
        "\n",
        "                ClustersDescription.at[0, col] = Description[:-2]\n",
        "\n",
        "            elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "                Q1 = round(DescStats.iloc[4, 0], decimal_points)\n",
        "                Q3 = round(DescStats.iloc[6, 0], decimal_points)\n",
        "                Description = f\"{Q1} -- {Q3}\"\n",
        "                ClustersDescription.at[0, col] = Description\n",
        "\n",
        "        except Exception as e:\n",
        "            ClustersDescription.at[0, col] = 'Not available'\n",
        "            print(\n",
        "                f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "\n",
        "    ClustersDescription['Cluster'] = str(cluster)\n",
        "\n",
        "    return ClustersDescription\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHo7wmH68AYc"
      },
      "source": [
        "Load a custom function to plot cluster distribution per Variable (absolute and relative levels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NN23X2dT8AeA"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "\n",
        "def cluster_distribution_per_variable(df, target):\n",
        "    \"\"\"\n",
        "    The data should have 2 variables, the cluster predictions and\n",
        "    the variable you want to analyze with, in this case we call \"target\".\n",
        "    We use plotly express to create 2 plots:\n",
        "    Cluster distribution across the target.\n",
        "    Relative presence of the target level in each cluster.\n",
        "    \"\"\"\n",
        "    df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index()\n",
        "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
        "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
        "\n",
        "    print(f\"Clusters distribution across {target} levels\")\n",
        "    fig = px.bar(df_bar_plot, x='Clusters', y='Count',\n",
        "                 color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.show(renderer='jupyterlab')\n",
        "\n",
        "    df_relative = (df\n",
        "                   .groupby([\"Clusters\", target])\n",
        "                   .size()\n",
        "                   .groupby(level=0)\n",
        "                   .apply(lambda x:  100*x / x.sum())\n",
        "                   .reset_index()\n",
        "                   .sort_values(by=['Clusters'])\n",
        "                   )\n",
        "    df_relative.columns = ['Clusters', target, 'Relative Percentage (%)']\n",
        "\n",
        "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
        "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\n",
        "                  color=target, width=800, height=500)\n",
        "    fig.update_layout(xaxis=dict(tickmode='array',\n",
        "                      tickvals=df['Clusters'].unique()))\n",
        "    fig.update_traces(mode='markers+lines')\n",
        "    fig.show(renderer='jupyterlab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73J7J65v4O_d"
      },
      "source": [
        "Create a DataFrame that contains best features and Clusters Predictions since we want to analyse the patterns for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PztdhjGl4Vkg"
      },
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "print(df_cluster_profile.shape)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mfJRrFc7wzu"
      },
      "source": [
        "We want also to analyse distribution of Multi Engine and Engine Types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSRSNqiF4mnm"
      },
      "outputs": [],
      "source": [
        "df_multi_engine = pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\").filter(['Multi_Engine'])\n",
        "df_multi_engine['Multi_Engine'] = df_multi_engine['Multi_Engine'].astype('object')\n",
        "df_multi_engine.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also testing for Engine Type albeit not the aim for the exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engine_type = pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\").filter(['Engine_Type'])\n",
        "df_engine_type['Engine_Type'] = df_engine_type['Engine_Type'].astype('object')\n",
        "df_engine_type.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtD0Y3NdJOhm"
      },
      "source": [
        "### Cluster profile based on the best features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For Multi Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDhycaSEdORm"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile_multi_engine = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_multi_engine], axis=1), decimal_points=0)\n",
        "clusters_profile_multi_engine.to_csv(f\"outputs/ml_pipeline/cluster_analysis/v1/clusters_profile_multi_engine.csv\")\n",
        "clusters_profile_multi_engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and for Engine Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile_engine_type = DescriptionAllClusters(df=pd.concat([df_cluster_profile,df_engine_type], axis=1), decimal_points=0)\n",
        "clusters_profile_engine_type.to_csv(f\"outputs/ml_pipeline/cluster_analysis/v1/clusters_profile_engine_type.csv\")\n",
        "clusters_profile_engine_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above Cluster Profile is very important since we from this plot can draw conclusions of the patterns.\n",
        "\n",
        "The description of the clusters tells us not surprisingly that the modification to the Engine Performance (TP_mods) was perfectly clustered and that Airplanes without TP mods are pretty much fifty-fifty when it comes to Multi Engine. Furthermore it shows that the TP mods all most only appear on Piston Engines.\n",
        "\n",
        "The most important conclusion from this clustering exercise is that TP mods appear to be:\n",
        "* Exclusively made on piston powered airplanes!\n",
        "* more common on Multi Engine Airplanes where 2/3 of the TP mods being done on Multi Engines.\n",
        "\n",
        "Since only one feature has been retained together with that the value of this clustering exercise has proved to be disappointing makes us not continue any furter with a re-fitting of the pipeline.\n",
        "\n",
        "The reason that this clustering did not yield much valuable insight is most likely becaus the the data set was to homogenous in terms of airplane types and that the airplanes in the data set was quite evenly and continuously distributed in terms of features such as size for example. If we would instead would have dropped the mid sized airplanes or introduced another type of airplanes such as fighter jets or drones we would most likely have experienced a more clear clustering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SS6CCCb74lH"
      },
      "source": [
        "### Clusters distribution across Multi Engine and Engine Type distributions & Relative Percentage of Multi Engine and Engine Type in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_multi_engine=  df_multi_engine.copy()\n",
        "df_cluster_vs_multi_engine['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_multi_engine, target='Multi_Engine')\n",
        "plt.savefig(f\"outputs/ml_pipeline/cluster_analysis/v1/cluster_vs_multi_engine.png\", bbox_inches='tight', dpi=150) # Save plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_cluster_vs_engine_type=  df_engine_type.copy()\n",
        "df_cluster_vs_engine_type['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_engine_type, target='Engine_Type')\n",
        "plt.savefig(f\"outputs/ml_pipeline/cluster_analysis/v1/cluster_vs_engine_type.png\", bbox_inches='tight', dpi=150) # Save plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxpjktKd1n6"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i9X1oOORAQc"
      },
      "source": [
        "\n",
        "We will generate the following files\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ySBIrV1Q4cY"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y9-0fisd5cl"
      },
      "source": [
        "## Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfv9k5xMd7fv"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsphnIR84hJ4"
      },
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_cluster, filename=f\"{file_path}/cluster_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ORnkwG6d74O"
      },
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqcwHaVwd9Ff"
      },
      "outputs": [],
      "source": [
        "df_cluster_study=df\n",
        "print(df.shape)\n",
        "df_cluster_study.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "M26MiJ9Y485Q"
      },
      "outputs": [],
      "source": [
        "df_cluster_study.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX_lcQVXaV0p"
      },
      "source": [
        "## Most important features plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9datNfsLCVV"
      },
      "source": [
        "These are the features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYeoH7fjaV8J"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr0cVVQsaZqk"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight', dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RObeac1HQq5a"
      },
      "source": [
        "## Cluster silhouette plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\n",
        "visualizer.fit(df_analysis)\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(7,5))\n",
        "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick', ax=axes)\n",
        "fig.fit(df_analysis)\n",
        "\n",
        "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight',dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the profile results we can label the two clusters in the following fashion:\n",
        "* **Cluster 0** represent airplanes that does not have TP mods and is made up by all Engine Types (piston, propjet and jet).\n",
        "* **Cluster 1** represent airplanes that has TP mods and is almost exclusively made up by Piston powered Airplanes\n",
        "(Airplane Type: piston) indicating that the TP mods is a feature applicable only on Piston Powered Engines. Furthermore Multi Engine Airplanes represented 2/3 of all TP mods."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modeling and Evaluation - Cluster Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
