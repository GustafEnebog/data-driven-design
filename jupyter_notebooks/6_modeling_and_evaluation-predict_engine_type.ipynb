{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a classification model to predict what Engine Type an airplane design has.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. They are found in each respective notebook.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Train set (features and target)\n",
        "* Test set (features and target)\n",
        "* Data cleaning and Feature Engineering pipeline\n",
        "* Modeling pipeline\n",
        "* Feature importance plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/data-driven-design/jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make the parent of the current directory the new current directory.\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/data-driven-design'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpFreVRiuM3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Step 1: Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(857, 21)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wing_Span</th>\n",
              "      <th>Length</th>\n",
              "      <th>Height</th>\n",
              "      <th>Multi_Engine</th>\n",
              "      <th>TP_mods</th>\n",
              "      <th>Engine_Type</th>\n",
              "      <th>AUW</th>\n",
              "      <th>MEW</th>\n",
              "      <th>FW</th>\n",
              "      <th>Vmax</th>\n",
              "      <th>...</th>\n",
              "      <th>Vstall</th>\n",
              "      <th>Range</th>\n",
              "      <th>Hmax</th>\n",
              "      <th>Hmax_(One)</th>\n",
              "      <th>ROC</th>\n",
              "      <th>ROC_(One)</th>\n",
              "      <th>Vlo</th>\n",
              "      <th>Slo</th>\n",
              "      <th>Vl</th>\n",
              "      <th>Sl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37.416667</td>\n",
              "      <td>25.250000</td>\n",
              "      <td>10.250000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>2050.0</td>\n",
              "      <td>1180.0</td>\n",
              "      <td>241.2</td>\n",
              "      <td>104.0</td>\n",
              "      <td>...</td>\n",
              "      <td>46.0</td>\n",
              "      <td>370.0</td>\n",
              "      <td>13000.0</td>\n",
              "      <td>13000.0</td>\n",
              "      <td>450.0</td>\n",
              "      <td>450.0</td>\n",
              "      <td>900.0</td>\n",
              "      <td>391.970247</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>257.745075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36.083333</td>\n",
              "      <td>20.583333</td>\n",
              "      <td>8.750000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1350.0</td>\n",
              "      <td>820.0</td>\n",
              "      <td>100.5</td>\n",
              "      <td>89.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>12300.0</td>\n",
              "      <td>12300.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>26.247647</td>\n",
              "      <td>800.0</td>\n",
              "      <td>225.324824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>21.416667</td>\n",
              "      <td>8.583333</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>810.0</td>\n",
              "      <td>127.3</td>\n",
              "      <td>90.0</td>\n",
              "      <td>...</td>\n",
              "      <td>37.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>16000.0</td>\n",
              "      <td>16000.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>475.0</td>\n",
              "      <td>363.139711</td>\n",
              "      <td>850.0</td>\n",
              "      <td>585.751317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Wing_Span     Length     Height  Multi_Engine  TP_mods  Engine_Type  \\\n",
              "0  37.416667  25.250000  10.250000         False    False            0   \n",
              "1  36.083333  20.583333   8.750000         False    False            0   \n",
              "2  35.000000  21.416667   8.583333         False    False            0   \n",
              "\n",
              "      AUW     MEW     FW   Vmax  ...  Vstall  Range     Hmax  Hmax_(One)  \\\n",
              "0  2050.0  1180.0  241.2  104.0  ...    46.0  370.0  13000.0     13000.0   \n",
              "1  1350.0   820.0  100.5   89.0  ...    44.0  190.0  12300.0     12300.0   \n",
              "2  1300.0   810.0  127.3   90.0  ...    37.0  210.0  16000.0     16000.0   \n",
              "\n",
              "     ROC  ROC_(One)    Vlo         Slo      Vl          Sl  \n",
              "0  450.0      450.0  900.0  391.970247  1300.0  257.745075  \n",
              "1  600.0      600.0  720.0   26.247647   800.0  225.324824  \n",
              "2  650.0      650.0  475.0  363.139711   850.0  585.751317  \n",
              "\n",
              "[3 rows x 21 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\")\n",
        "      .drop(labels=['Model', 'Company', 'THR', 'SHP'], axis=1)  # Should I also drop 'Engine Type'\n",
        "                    # target variable for regressor, remove from classifier  \n",
        "                    # drop other variables we will not need for this project\n",
        "  )\n",
        "\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofil7xTpm6l9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Step 2: ML Pipeline with all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfCsXhBYVBJw"
      },
      "source": [
        "## ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "    # Specify variables for Ordinal Encoder and Power Transformer\n",
        "    ordinal_variables = ['Multi_Engine','TP_mods', 'Engine_Type']\n",
        "    \n",
        "    # Specify the numerical variables for Yeo-Johnson transformation\n",
        "    yeo_johnson_variables = ['Wing_Span', 'Length', 'Height', 'AUW', 'MEW',\n",
        "                             'FW', 'Vmax', 'Vcruise', 'Vstall', 'Range', \n",
        "                             'Hmax', 'Hmax (One)', 'ROC', 'ROC (One)',\n",
        "                             'Vlo', 'Slo', 'Vl', 'Sl']\n",
        "\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(variables=ordinal_variables)),\n",
        "        \n",
        "        (\"YeoJohnsonTransformer\", PowerTransformer(method='yeo-johnson')),\n",
        "        \n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=yeo_johnson_variables,\n",
        "         method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "pipeline = PipelineDataCleaningAndFeatureEngineering()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_7BXNYMULrf"
      },
      "source": [
        "## ML Pipeline for Modelling and Hyperparameter Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PYR4hz6-Ldvo"
      },
      "outputs": [],
      "source": [
        "# Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "def PipelineClf(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"feat_selection\", SelectFromModel(model)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM_hrtfjLj85"
      },
      "source": [
        "Custom Class for Hyperparameter Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NpTcVDtQ5RMc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "\n",
        "            model = PipelineClf(self.models[key])\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "        return df[columns], self.grid_searches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUcOp83jy0QG"
      },
      "source": [
        "## Split Train and Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the shapes shows that 80% is taken for a train and the rest (20%) is for test. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Multi_Engine'] = df['Multi_Engine'].astype(object)\n",
        "df['TP_mods'] = df['TP_mods'].astype(object)\n",
        "df['Engine_Type'] = df['Engine_Type'].astype(object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0vqzNI2zF1sZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(857, 21)\n",
            "(685, 20) (685,) (172, 20) (172,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(['Multi_Engine'], axis=1),\n",
        "    df['Multi_Engine'],\n",
        "    test_size=0.2,\n",
        "    random_state=0,\n",
        ")\n",
        "print(df.shape)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "# df = pd.DataFrame(y_train.shape)\n",
        "# print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "685 + 172 = 857 perfect result from the split of the data set (75% train and 25% test)!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Wing_Span     Length     Height Multi_Engine TP_mods Engine_Type     AUW  \\\n",
            "0    37.416667  25.250000  10.250000        False   False           0  2050.0   \n",
            "1    36.083333  20.583333   8.750000        False   False           0  1350.0   \n",
            "2    35.000000  21.416667   8.583333        False   False           0  1300.0   \n",
            "3    35.000000  21.416667   8.583333        False   False           0  1300.0   \n",
            "4    35.000000  21.416667   8.750000        False   False           0  1220.0   \n",
            "..         ...        ...        ...          ...     ...         ...     ...   \n",
            "852  49.416667  35.083333  14.416667         True   False           0  6250.0   \n",
            "853  49.000000  35.083333  14.416667         True   False           0  6000.0   \n",
            "854  44.083333  35.416667  14.416667         True   False           0  5500.0   \n",
            "855  30.000000  21.166667   7.000000        False   False           0  1817.0   \n",
            "856  30.000000  21.166667   7.000000        False   False           0  1817.0   \n",
            "\n",
            "        MEW      FW   Vmax  ...  Vstall   Range     Hmax  Hmax_(One)     ROC  \\\n",
            "0    1180.0   241.2  104.0  ...    46.0   370.0  13000.0     13000.0   450.0   \n",
            "1     820.0   100.5   89.0  ...    44.0   190.0  12300.0     12300.0   600.0   \n",
            "2     810.0   127.3   90.0  ...    37.0   210.0  16000.0     16000.0   650.0   \n",
            "3     800.0   127.3   88.0  ...    37.0   210.0  13000.0     13000.0   620.0   \n",
            "4     740.0    93.8   83.0  ...    33.0   175.0  12500.0     12500.0   370.0   \n",
            "..      ...     ...    ...  ...     ...     ...      ...         ...     ...   \n",
            "852  4255.0  1045.2  198.0  ...    54.0   957.0  22500.0      7850.0  1400.0   \n",
            "853  3850.0  1045.2  190.0  ...    55.0   957.0  22500.0      7150.0  1400.0   \n",
            "854  3800.0  1005.0  183.0  ...    52.0  1000.0  24000.0      8500.0  1700.0   \n",
            "855  1175.0   221.1  130.0  ...    45.0   365.0  22000.0     22000.0  1310.0   \n",
            "856  1125.0   221.1  117.0  ...    45.0   380.0  22000.0     22000.0   910.0   \n",
            "\n",
            "     ROC_(One)     Vlo          Slo      Vl          Sl  \n",
            "0        450.0   900.0   391.970247  1300.0  257.745075  \n",
            "1        600.0   720.0    26.247647   800.0  225.324824  \n",
            "2        650.0   475.0   363.139711   850.0  585.751317  \n",
            "3        620.0   500.0   407.797297   850.0  642.046166  \n",
            "4        370.0   632.0   297.056192   885.0  329.571813  \n",
            "..         ...     ...          ...     ...         ...  \n",
            "852      320.0  1210.0   970.000000  1150.0  865.000000  \n",
            "853      290.0  1250.0  1000.000000  1350.0  950.000000  \n",
            "854      400.0  1100.0   950.000000  1050.0  900.000000  \n",
            "855     1310.0  1730.0  1120.000000  1585.0  785.000000  \n",
            "856      910.0  1730.0  1120.000000  1585.0  785.000000  \n",
            "\n",
            "[857 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Wing_Span     Length     Height TP_mods Engine_Type      AUW      MEW  \\\n",
            "196  64.333333  68.416667  20.666667   False           2  48300.0  26985.0   \n",
            "645  37.166667  31.166667  10.250000   False           0   5200.0   3229.0   \n",
            "122  37.666667  25.583333   9.500000   False           0   4880.0   2960.0   \n",
            "851  49.416667  35.083333  14.750000   False           0   6750.0   4350.0   \n",
            "167  34.166667  26.333333   7.333333   False           0   3325.0   2247.0   \n",
            "..         ...        ...        ...     ...         ...      ...      ...   \n",
            "835  44.083333  43.000000  14.500000   False           1   9400.0   5515.0   \n",
            "192  61.833333  68.416667  20.666667   False           2  41400.0  18660.0   \n",
            "629  53.250000  47.250000  14.000000   False           1   9920.0   5732.0   \n",
            "559  28.750000  20.333333   7.416667   False           0   1750.0   1160.0   \n",
            "684  36.166667  27.833333   9.500000   False           0   3600.0   2222.0   \n",
            "\n",
            "          FW    Vmax  Vcruise     Vstall   Range          Hmax    Hmax_(One)  \\\n",
            "196  20000.0  546.94    533.6  99.659388  4045.0  41000.000000  20000.000000   \n",
            "645    964.8  220.00    197.0  59.000000   620.0  30000.000000  18700.000000   \n",
            "122    750.4  200.00    191.0  66.000000   745.0  19200.000000   7600.000000   \n",
            "851   1045.2  198.00    190.0  59.000000   957.0  24000.000000   8500.000000   \n",
            "167    455.6  174.00    165.0  61.000000   483.0  18200.000000  18200.000000   \n",
            "..       ...     ...      ...        ...     ...           ...           ...   \n",
            "835   1916.2  252.00    236.0  89.000000  1296.0  25600.000000  10500.000000   \n",
            "192  14890.0  480.00    442.0  93.912157  2991.0  41000.000000  24000.000000   \n",
            "629   2693.4  270.00    265.0  64.000000  2261.0  30000.000000  30000.000000   \n",
            "559    201.0  162.00    125.0  49.000000   505.0  17747.603162  17747.603162   \n",
            "684    683.4  155.00    148.0  59.000000   804.0  17200.000000  17200.000000   \n",
            "\n",
            "             ROC    ROC_(One)          Vlo          Slo           Vl  \\\n",
            "196  2512.569226   680.000000  6969.145819  5684.000000  3693.285251   \n",
            "645  1530.000000   265.000000  1250.000000   820.000000  1250.000000   \n",
            "122  1630.000000   350.000000  1700.000000  1255.000000  1470.000000   \n",
            "851  1700.000000   340.000000  1375.000000  1100.000000  1235.000000   \n",
            "167  1170.000000  1170.000000  1420.000000   980.000000  1340.000000   \n",
            "..           ...          ...          ...          ...          ...   \n",
            "835  2007.000000   484.000000  2016.000000  1706.000000  2504.000000   \n",
            "192  3400.000000   743.000000  5700.000000  4703.980789  3300.000000   \n",
            "629  1680.000000  1680.000000  2300.000000  1475.000000  1830.000000   \n",
            "559   682.000000   682.000000  1496.000000   822.000000  1519.000000   \n",
            "684  1007.400328  1007.400328  2028.000000  1284.000000  1822.000000   \n",
            "\n",
            "              Sl  \n",
            "196  2777.000000  \n",
            "645   850.000000  \n",
            "122  1250.000000  \n",
            "851   865.000000  \n",
            "167   835.000000  \n",
            "..           ...  \n",
            "835  1200.000000  \n",
            "192  2999.403043  \n",
            "629   945.000000  \n",
            "559   841.000000  \n",
            "684   911.000000  \n",
            "\n",
            "[685 rows x 20 columns]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "196     True\n",
            "645     True\n",
            "122     True\n",
            "851     True\n",
            "167    False\n",
            "       ...  \n",
            "835     True\n",
            "192     True\n",
            "629    False\n",
            "559    False\n",
            "684    False\n",
            "Name: Multi_Engine, Length: 685, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Wing_Span     Length     Height TP_mods Engine_Type      AUW      MEW  \\\n",
            "144  32.750000  25.083333   6.500000   False           0   2650.0   1575.0   \n",
            "774  43.000000  30.083333  11.250000   False           1   4318.0   2900.0   \n",
            "263  44.083333  36.333333  11.416667   False           0   6850.0   4238.0   \n",
            "788  44.333333  43.750000  16.000000   False           2  19612.0  10500.0   \n",
            "846  49.000000  35.083333  14.416667   False           0   6000.0   4250.0   \n",
            "..         ...        ...        ...     ...         ...      ...      ...   \n",
            "156  32.750000  25.666667   8.250000   False           0   2750.0   1711.0   \n",
            "597  36.416667  26.750000   8.333333   False           0   3374.0   2370.0   \n",
            "624  34.750000  27.416667   8.500000   False           0   3315.0   2000.0   \n",
            "50   36.750000  29.583333  10.333333   False           0   3800.0   2588.0   \n",
            "822  36.000000  22.250000   6.500000   False           0   1500.0    990.0   \n",
            "\n",
            "         FW   Vmax  Vcruise  Vstall   Range     Hmax  Hmax_(One)     ROC  \\\n",
            "144   261.3  160.0    148.0    49.0   470.0  17100.0     17100.0   890.0   \n",
            "774  1011.7  265.0    262.0    60.0  1000.0  27000.0     27000.0  3000.0   \n",
            "263  1427.1  230.0    213.0    68.0   875.0  26900.0     14800.0  1450.0   \n",
            "788  7122.0  490.0    430.0    83.0  1640.0  45000.0     20500.0  4700.0   \n",
            "846  1045.2  193.0    184.0    58.0   826.0  22000.0      9750.0  1510.0   \n",
            "..      ...    ...      ...     ...     ...      ...         ...     ...   \n",
            "156   348.4  140.0    131.0    55.0   591.0  14342.0     14342.0   891.0   \n",
            "597   670.0  242.0    237.0    59.0  1852.0  25000.0     25000.0  1380.0   \n",
            "624   268.0  177.0    166.0    48.0   335.0  21500.0     21500.0  1375.0   \n",
            "50    536.0  168.0    160.0    63.0   680.0  17400.0      4250.0  1160.0   \n",
            "822   160.8  109.0    103.0    37.0   296.0  18000.0     18000.0   875.0   \n",
            "\n",
            "     ROC_(One)          Vlo          Slo           Vl           Sl  \n",
            "144      890.0  1515.000000  1275.000000   950.000000   625.000000  \n",
            "774     3000.0  1200.000000   800.000000  1000.000000   800.000000  \n",
            "263      301.0  2195.000000  1763.000000  2485.000000  1055.000000  \n",
            "788     1200.0  4900.000000  2974.251395  2350.000000  2306.123234  \n",
            "846      380.0  1250.000000  1050.000000  1370.000000   960.000000  \n",
            "..         ...          ...          ...          ...          ...  \n",
            "156      891.0  1804.000000  1169.000000  1519.000000   803.000000  \n",
            "597     1380.0  1961.261049  1378.000000  1847.483968  2198.000000  \n",
            "624     1375.0   980.000000   737.000000   980.000000   763.000000  \n",
            "50       200.0  1805.000000  1000.000000  1330.000000   710.000000  \n",
            "822      875.0   350.000000   275.000000   350.000000   753.944239  \n",
            "\n",
            "[172 rows x 20 columns]\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144    False\n",
            "774    False\n",
            "263     True\n",
            "788     True\n",
            "846     True\n",
            "       ...  \n",
            "156    False\n",
            "597    False\n",
            "624    False\n",
            "50      True\n",
            "822    False\n",
            "Name: Multi_Engine, Length: 172, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example\n",
        "y_train = pd.Series([...])  # Replace with your actual target data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zBysp0tyqR2"
      },
      "source": [
        "## Handle Target Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We already see upfront on the Kaggle site for this data that the 'Multi Engine' categorical data is in need of rebalancing\n",
        "\n",
        "\n",
        "<img src=\"/workspace/data-driven-design/images_notebook/cat_data_distr_kaggle.png\" alt=\"Information on three categorical features showing the degree of target imbalance\" height=\"200\" />\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MsQRvnn1GI_d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [685, 1]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline_data_cleaning_feat_eng \u001b[38;5;241m=\u001b[39m PipelineDataCleaningAndFeatureEngineering()\n\u001b[0;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_data_cleaning_feat_eng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pipeline_data_cleaning_feat_eng\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape, y_train\u001b[38;5;241m.\u001b[39mshape, X_test\u001b[38;5;241m.\u001b[39mshape, y_test\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03mFits all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/pipeline.py:377\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    375\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 957\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/base.py:919\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/encoding/ordinal.py:197\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Learn the numbers to be used to replace the categories in each\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03mvariable.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    Otherwise, y needs to be passed when fitting the transformer.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mordered\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 197\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_X(X)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/dataframe_checks.py:194\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, multi_output, y_numeric)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# case 1: both are pandas objects\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, pd\u001b[38;5;241m.\u001b[39mSeries):\n\u001b[0;32m--> 194\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Check that their indexes match.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mequals(y\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/dataframe_checks.py:189\u001b[0m, in \u001b[0;36mcheck_X_y.<locals>._check_X_y\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m X \u001b[38;5;241m=\u001b[39m check_X(X)\n\u001b[1;32m    188\u001b[0m y \u001b[38;5;241m=\u001b[39m check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m--> 189\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [685, 1]"
          ]
        }
      ],
      "source": [
        "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
        "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train,y_train)\n",
        "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuq3902arZAz"
      },
      "source": [
        "Check Train Set Target distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I28ACrp-rPgF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OgoR6lTrKqY"
      },
      "source": [
        "Use SMOTE (Synthetic Minority Oversampling TEchnique) to balance Train Set target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP1JIwXNEsXO"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTJO6V5zrdnw"
      },
      "source": [
        "Check Train Set Target distribution after resampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQdvEvNRG80Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xTTXMayvo6"
      },
      "source": [
        "## Grid Search CV - Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fizLJ_YQ6elb"
      },
      "source": [
        "### Use standard hyperparameters to find most suitable algorithm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kMgswohfKBda"
      },
      "outputs": [],
      "source": [
        "models_quick_search = {\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=0),\n",
        "    \"XGBClassifier\": XGBClassifier(random_state=0),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=0),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(random_state=0),\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=0),\n",
        "    \"ExtraTreesClassifier\": ExtraTreesClassifier(random_state=0),\n",
        "    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=0),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    \"LogisticRegression\": {},\n",
        "    \"XGBClassifier\": {},\n",
        "    \"DecisionTreeClassifier\": {},\n",
        "    \"RandomForestClassifier\": {},\n",
        "    \"GradientBoostingClassifier\": {},\n",
        "    \"ExtraTreesClassifier\": {},\n",
        "    \"AdaBoostClassifier\": {},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXu0Ryeown7N"
      },
      "source": [
        "Quick GridSearch CV - Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7eLJcKEKBlQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, recall_score\n",
        "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0bkL-IxwnJx"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpFOc7OAKMuz"
      },
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewezVDt46jTJ"
      },
      "source": [
        "### Do an extensive search on the most suitable algorithm to find the best hyperparameter configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1WozH5frBQ9"
      },
      "source": [
        "Define model and parameters, for Extensive Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sDT_WMUErBRB"
      },
      "outputs": [],
      "source": [
        "models_search = {\n",
        "    \"XGBClassifier\":XGBClassifier(random_state=0),\n",
        "}\n",
        "\n",
        "# documentation to help on hyperparameter list: \n",
        "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
        "\n",
        "# We will not conduct an extensive search, since the focus\n",
        "# is on how to combine all knowledge in an applied project.\n",
        "# In a workplace project, you may spend more time in this step\n",
        "params_search = {\n",
        "    \"XGBClassifier\":{\n",
        "        'model__learning_rate': [1e-1,1e-2,1e-3], \n",
        "        'model__max_depth': [3,10,None],\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP2Ua0FGrBRC"
      },
      "source": [
        "Extensive GridSearch CV - Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK1s893orBRD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score, make_scorer\n",
        "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8oVKtHyr-X8"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AFyZ6-pr9tN"
      },
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get best model name programmatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htAXEVFpwiBV"
      },
      "source": [
        "Parameters for best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDIt27RdKOG8"
      },
      "outputs": [],
      "source": [
        "best_parameters = grid_search_pipelines[best_model].best_params_\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAnJQlDlw1FE"
      },
      "source": [
        "Define the best clf pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLotNfy4MKDE"
      },
      "outputs": [],
      "source": [
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgdxKijH6qJS"
      },
      "source": [
        "## Assess feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n30pl2dowzW3"
      },
      "source": [
        "* With the current model, we can assess with `.features_importances_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XGczhv2uo2C"
      },
      "outputs": [],
      "source": [
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': X_train.columns[pipeline_clf['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# re-assign best_features order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "## Evaluate Pipeline on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myG6tDSGan4r"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
        "\n",
        "    prediction = pipeline.predict(X)\n",
        "\n",
        "    print('---  Confusion Matrix  ---')\n",
        "    print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
        "          columns=[[\"Actual \" + sub for sub in label_map]],\n",
        "          index=[[\"Prediction \" + sub for sub in label_map]]\n",
        "          ))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print('---  Classification Report  ---')\n",
        "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
        "\n",
        "\n",
        "def clf_performance(X_train, y_train, X_test, y_test, pipeline, label_map):\n",
        "    print(\"#### Train Set #### \\n\")\n",
        "    confusion_matrix_and_report(X_train, y_train, pipeline, label_map)\n",
        "\n",
        "    print(\"#### Test Set ####\\n\")\n",
        "    confusion_matrix_and_report(X_test, y_test, pipeline, label_map)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpUfEAGlW5aK"
      },
      "source": [
        "Evaluation: We cross check with metrics defined at ML business case\n",
        "* 80% Recall for Churn, on train and test set\n",
        "* 80% Precision for no Churn on train and test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umWjIvGMNLig"
      },
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_clf,\n",
        "                label_map= ['No Churn', 'Churn'] \n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WgttWjtHHOQ"
      },
      "source": [
        "# Step 3: Refit pipeline with best features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCyOyebVHVmA"
      },
      "source": [
        "## Refit ML Pipeline and Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4PpI2sKC5IL"
      },
      "source": [
        "In theory, a pipeline fitted **using only the most important features** should give the same result as the one fitted with **all variables and feature selection**\n",
        "\n",
        "* However, in this project we have a step for feature augmentation, which is to balance the target Train Set using SMOTE().\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km_-hW0f68DP"
      },
      "source": [
        "## Rewrite ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBeckIjkCa4k"
      },
      "source": [
        "New Pipeline for DataCleaning And FeatureEngineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc8ptvFiHJmb"
      },
      "outputs": [],
      "source": [
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['InternetService', 'Contract'])),\n",
        "\n",
        "\n",
        "        # we don't need SmartCorrelatedSelection\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGNs9PU16_Ls"
      },
      "source": [
        "## Rewrite ML Pipeline for Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpjmxzTbCXlg"
      },
      "source": [
        "Function for Pipeline optmisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E76QmoMEWA0"
      },
      "outputs": [],
      "source": [
        "# Pipeline Optmization: Model\n",
        "def PipelineClf(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        # no feature selection needed anymore!!! We know which features to use already!\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75hfh3o5GhoU"
      },
      "source": [
        "## Split Train Test Set, considering only with best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6dX0VeKGhod"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(['Churn'], axis=1),\n",
        "    df['Churn'],\n",
        "    test_size=0.2,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19a3t6jI6H6"
      },
      "source": [
        "We filter only the most important variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5Acb9T_GXjU"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.filter(best_features)\n",
        "X_test = X_test.filter(best_features)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjOcRGheGhof"
      },
      "source": [
        "## Handle Target Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbQda_pcGhof"
      },
      "outputs": [],
      "source": [
        "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
        "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train)\n",
        "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxIFw3KGhog"
      },
      "source": [
        "Check Train Set Target distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZQyth-2Ghog"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9FbCbIrGhoh"
      },
      "source": [
        "Use SMOTE to balance Train Set target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtbWft5VGhoh"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YozwzI9eGhoh"
      },
      "source": [
        "Check Train Set Target distribution after SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyL99cYMGhoi"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts().plot(kind='bar',title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_WjvD_QIJ_L"
      },
      "source": [
        "## Grid Search CV: Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESkqrySI7N6u"
      },
      "source": [
        "Using the most suitable model from the last section and its best hyperparameter configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fFaXDOIJ_M"
      },
      "source": [
        "We are using the same model from  the last GridCV search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7F0z__h1qSA"
      },
      "outputs": [],
      "source": [
        "models_search   # XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRteBPgd3ldU"
      },
      "source": [
        "And the best parameters from the last GridCV search "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbGNBeZk3V8r"
      },
      "outputs": [],
      "source": [
        "best_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlLJP5Ds3rYp"
      },
      "source": [
        "You will need to type in manually since the hyperparameter values have to be a list. The previous dictionary is not in this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bC8RmE-2Mi2"
      },
      "outputs": [],
      "source": [
        "params_search = {'XGBClassifier':  {\n",
        "    'model__learning_rate': [0.01],   # the value should be in []\n",
        "    'model__max_depth': [3]},  # the value should be in []\n",
        "}\n",
        "params_search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrRJNywsIJ_M"
      },
      "source": [
        "GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv5nO6cJP9fX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score, make_scorer\n",
        "quick_search = HyperparameterOptimizationSearch(\n",
        "    models=models_search, params=params_search)\n",
        "quick_search.fit(X_train, y_train,\n",
        "                 scoring=make_scorer(recall_score, pos_label=1),\n",
        "                 n_jobs=-1, cv=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_Yu9ykIJ_N"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqIk1g95IJ_N"
      },
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZcP3yXpIJ_O"
      },
      "source": [
        "Define the best clf pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7Qe2jFEIJ_O"
      },
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0, 0]\n",
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEXWvsb7Su0"
      },
      "source": [
        "## Assess feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8UGZ5bnIJ_P"
      },
      "outputs": [],
      "source": [
        "best_features = X_train.columns\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': best_features,\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQF20xan7VuK"
      },
      "source": [
        "## Evaluate Pipeline on Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation: We cross-check with metrics defined in the ML business case.\n",
        "* 80% Recall for Churn, on train and test set.\n",
        "* 80% Precision for no Churn on train and test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cpCj2lLHxB-"
      },
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_clf,\n",
        "                label_map= ['No Churn', 'Churn'] \n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBVunRgBqIXQ"
      },
      "source": [
        "# Step 4: Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxnlKI5SJcoO"
      },
      "source": [
        "We will generate the following files\n",
        "* Train set\n",
        "* Test set\n",
        "* Data cleaning and Feature Engineering pipeline\n",
        "* Modeling pipeline\n",
        "* features importance plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16bIOgs3J7OD"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/predict_churn/{version}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e-gC6sa7hpj"
      },
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHZUZKJ5JiKn"
      },
      "source": [
        "* note that the variables **are transformed already** in X_train and the shape is 8266 - after SMOTE was applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc4fzrdTJno1"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qzq7DgVTJnv3"
      },
      "outputs": [],
      "source": [
        "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzPsdNGX9gtf"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMoT1cJ39g26"
      },
      "outputs": [],
      "source": [
        "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYatlgsj7pbB"
      },
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEKp3-dJJn3p"
      },
      "source": [
        "* note that the variables are transformed already in X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UMg2vPtJqxM"
      },
      "outputs": [],
      "source": [
        "print(X_test.shape)\n",
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz2OqPW6Jqzv"
      },
      "outputs": [],
      "source": [
        "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pPTVz219xj3"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap7fYYAm9xsj"
      },
      "outputs": [],
      "source": [
        "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ufHAplN7tdo"
      },
      "source": [
        "## ML Pipelines: Data Cleaning and Feat Eng pipeline and Modelling Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAbbAO2r248W"
      },
      "source": [
        "We will save 2 pipelines: \n",
        "* Both should be used in conjunction to predict Live Data.\n",
        "* To predict on Train Set, Test Set we use only pipeline_clf, since the data is already processed.\n",
        "\n",
        "\n",
        "\n",
        "Pipeline responsible for Data Cleaning and Feature Engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCcAlvoG3CRm"
      },
      "outputs": [],
      "source": [
        "pipeline_data_cleaning_feat_eng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaHdCf4HKBLg"
      },
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_data_cleaning_feat_eng ,\n",
        "            filename=f\"{file_path}/clf_pipeline_data_cleaning_feat_eng.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE-iU6TL3LVI"
      },
      "source": [
        "* Pipeline responsible for Feature Scaling, and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zEBxfvBqI29"
      },
      "outputs": [],
      "source": [
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObL5Iz8tKdsZ"
      },
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_clf ,\n",
        "            filename=f\"{file_path}/clf_pipeline_model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqEUyLG27v9N"
      },
      "source": [
        "## Feature Importance plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBiqB55L1Qhk"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR0taWpn1RuD"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Good job, you should clear outputs, then run git commands to push files to the repo. Next, move on to Predict Tenure notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modeling and Evaluation - Predict Customer Churn.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
