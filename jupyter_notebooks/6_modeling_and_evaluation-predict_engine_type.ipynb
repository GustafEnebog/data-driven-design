{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "*   Fit and evaluate a classification model to predict what Engine Type an airplane design has.\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
        "* Instructions on which variables to use for data cleaning and feature engineering. They are found in each respective notebook.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Train set (features and target)\n",
        "* Test set (features and target)\n",
        "* Data cleaning and Feature Engineering pipeline\n",
        "* Modeling pipeline\n",
        "* Feature importance plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/data-driven-design/jupyter_notebooks'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make the parent of the current directory the new current directory.\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/data-driven-design'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpFreVRiuM3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Step 1: Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(857, 21)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wing_Span</th>\n",
              "      <th>Length</th>\n",
              "      <th>Height</th>\n",
              "      <th>Multi_Engine</th>\n",
              "      <th>TP_mods</th>\n",
              "      <th>Engine_Type</th>\n",
              "      <th>AUW</th>\n",
              "      <th>MEW</th>\n",
              "      <th>FW</th>\n",
              "      <th>Vmax</th>\n",
              "      <th>...</th>\n",
              "      <th>Vstall</th>\n",
              "      <th>Range</th>\n",
              "      <th>Hmax</th>\n",
              "      <th>Hmax_(One)</th>\n",
              "      <th>ROC</th>\n",
              "      <th>ROC_(One)</th>\n",
              "      <th>Vlo</th>\n",
              "      <th>Slo</th>\n",
              "      <th>Vl</th>\n",
              "      <th>Sl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37.416667</td>\n",
              "      <td>25.250000</td>\n",
              "      <td>10.250000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>2050.0</td>\n",
              "      <td>1180.0</td>\n",
              "      <td>241.2</td>\n",
              "      <td>104.0</td>\n",
              "      <td>...</td>\n",
              "      <td>46.0</td>\n",
              "      <td>370.0</td>\n",
              "      <td>13000.0</td>\n",
              "      <td>13000.0</td>\n",
              "      <td>450.0</td>\n",
              "      <td>450.0</td>\n",
              "      <td>900.0</td>\n",
              "      <td>391.970247</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>257.745075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36.083333</td>\n",
              "      <td>20.583333</td>\n",
              "      <td>8.750000</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1350.0</td>\n",
              "      <td>820.0</td>\n",
              "      <td>100.5</td>\n",
              "      <td>89.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>12300.0</td>\n",
              "      <td>12300.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>26.247647</td>\n",
              "      <td>800.0</td>\n",
              "      <td>225.324824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35.000000</td>\n",
              "      <td>21.416667</td>\n",
              "      <td>8.583333</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>810.0</td>\n",
              "      <td>127.3</td>\n",
              "      <td>90.0</td>\n",
              "      <td>...</td>\n",
              "      <td>37.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>16000.0</td>\n",
              "      <td>16000.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>650.0</td>\n",
              "      <td>475.0</td>\n",
              "      <td>363.139711</td>\n",
              "      <td>850.0</td>\n",
              "      <td>585.751317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Wing_Span     Length     Height  Multi_Engine  TP_mods  Engine_Type  \\\n",
              "0  37.416667  25.250000  10.250000         False    False            0   \n",
              "1  36.083333  20.583333   8.750000         False    False            0   \n",
              "2  35.000000  21.416667   8.583333         False    False            0   \n",
              "\n",
              "      AUW     MEW     FW   Vmax  ...  Vstall  Range     Hmax  Hmax_(One)  \\\n",
              "0  2050.0  1180.0  241.2  104.0  ...    46.0  370.0  13000.0     13000.0   \n",
              "1  1350.0   820.0  100.5   89.0  ...    44.0  190.0  12300.0     12300.0   \n",
              "2  1300.0   810.0  127.3   90.0  ...    37.0  210.0  16000.0     16000.0   \n",
              "\n",
              "     ROC  ROC_(One)    Vlo         Slo      Vl          Sl  \n",
              "0  450.0      450.0  900.0  391.970247  1300.0  257.745075  \n",
              "1  600.0      600.0  720.0   26.247647   800.0  225.324824  \n",
              "2  650.0      650.0  475.0  363.139711   850.0  585.751317  \n",
              "\n",
              "[3 rows x 21 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/airplane_performance_study.csv\")\n",
        "      .drop(labels=['Model', 'Company', 'THR', 'SHP'], axis=1)  # Should I also drop 'Engine Type'\n",
        "                    # target variable for regressor, remove from classifier  \n",
        "                    # drop other variables we will not need for this project\n",
        "  )\n",
        "\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofil7xTpm6l9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Step 2: ML Pipeline with all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfCsXhBYVBJw"
      },
      "source": [
        "## ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "    # Specify variables for Ordinal Encoder and Power Transformer\n",
        "    ordinal_variables = ['Multi_Engine','TP_mods']\n",
        "    \n",
        "    # Specify the numerical variables for Yeo-Johnson transformation\n",
        "    yeo_johnson_variables = ['Wing_Span', 'Length', 'Height', 'AUW', 'MEW',\n",
        "                             'FW', 'Vmax', 'Vcruise', 'Vstall', 'Range', \n",
        "                             'Hmax', 'Hmax (One)', 'ROC', 'ROC (One)',\n",
        "                             'Vlo', 'Slo', 'Vl', 'Sl']\n",
        "\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(variables=ordinal_variables)),\n",
        "        \n",
        "        (\"YeoJohnsonTransformer\", PowerTransformer(method='yeo-johnson')),\n",
        "        \n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=yeo_johnson_variables,\n",
        "         method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "pipeline = PipelineDataCleaningAndFeatureEngineering()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_7BXNYMULrf"
      },
      "source": [
        "## ML Pipeline for Modelling and Hyperparameter Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PYR4hz6-Ldvo"
      },
      "outputs": [],
      "source": [
        "# Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "def PipelineClf(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"feat_selection\", SelectFromModel(model)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM_hrtfjLj85"
      },
      "source": [
        "Custom Class for Hyperparameter Optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NpTcVDtQ5RMc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "\n",
        "            model = PipelineClf(self.models[key])\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "        return df[columns], self.grid_searches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "   \n",
        "   OLD stuff DELETE\n",
        "    df.drop(['Wing_Span', 'Length', 'Height', 'AUW', 'MEW', 'FW', 'Vmax', 'Vcruise', 'Vstall', 'Range', 'Hmax', 'Hmax_(One)', 'ROC', 'ROC_(One)', 'Vlo', 'Slo', 'Vl', 'Sl'], axis=1),\n",
        "    df[['Wing_Span', 'Length', 'Height', 'AUW', 'MEW', 'FW', 'Vmax', 'Vcruise', 'Vstall', 'Range', 'Hmax', 'Hmax_(One)', 'ROC', 'ROC_(One)', 'Vlo', 'Slo', 'Vl', 'Sl']],"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUcOp83jy0QG"
      },
      "source": [
        "## Split Train and Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the shapes shows that 80% is taken for a train and the rest (20%) is for test. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0vqzNI2zF1sZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(857, 21)\n",
            "(685, 20) (685, 18) (172, 20) (172, 18)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(['Engine_Type'], axis=1),\n",
        "    df[['Wing_Span', 'Length', 'Height', 'AUW', 'MEW', 'FW', 'Vmax', 'Vcruise', 'Vstall', 'Range', 'Hmax', 'Hmax_(One)', 'ROC', 'ROC_(One)', 'Vlo', 'Slo', 'Vl', 'Sl']],\n",
        "    test_size=0.2,\n",
        "    random_state=0,\n",
        ")\n",
        "print(df.shape)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zBysp0tyqR2"
      },
      "source": [
        "## Handle Target Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MsQRvnn1GI_d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "y should be a 1d array, got an array of shape (685, 18) instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline_data_cleaning_feat_eng \u001b[38;5;241m=\u001b[39m PipelineDataCleaningAndFeatureEngineering()\n\u001b[0;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_data_cleaning_feat_eng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pipeline_data_cleaning_feat_eng\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape, y_train\u001b[38;5;241m.\u001b[39mshape, X_test\u001b[38;5;241m.\u001b[39mshape, y_test\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03mFits all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/pipeline.py:377\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    375\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 957\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/base.py:919\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/encoding/ordinal.py:197\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Learn the numbers to be used to replace the categories in each\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03mvariable.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    Otherwise, y needs to be passed when fitting the transformer.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mordered\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 197\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_X(X)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/dataframe_checks.py:201\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, multi_output, y_numeric)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# case 2: X is dataframe and y is something else\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, pd\u001b[38;5;241m.\u001b[39mSeries):\n\u001b[0;32m--> 201\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     y\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# case 3: X is not a dataframe and y is a series\u001b[39;00m\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/dataframe_checks.py:188\u001b[0m, in \u001b[0;36mcheck_X_y.<locals>._check_X_y\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(X, y):\n\u001b[1;32m    187\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_X(X)\n\u001b[0;32m--> 188\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/feature_engine/dataframe_checks.py:136\u001b[0m, in \u001b[0;36mcheck_y\u001b[0;34m(y, multi_output, y_numeric)\u001b[0m\n\u001b[1;32m    133\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(y)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:1183\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1183\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[1;32m   1185\u001b[0m     _ensure_no_complex_data(y)\n",
            "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/utils/validation.py:1244\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1234\u001b[0m             (\n\u001b[1;32m   1235\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m-> 1244\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1246\u001b[0m )\n",
            "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (685, 18) instead."
          ]
        }
      ],
      "source": [
        "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
        "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train, y_train)\n",
        "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuq3902arZAz"
      },
      "source": [
        "Check Train Set Target distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I28ACrp-rPgF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OgoR6lTrKqY"
      },
      "source": [
        "Use SMOTE (Synthetic Minority Oversampling TEchnique) to balance Train Set target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP1JIwXNEsXO"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTJO6V5zrdnw"
      },
      "source": [
        "Check Train Set Target distribution after resampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQdvEvNRG80Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xTTXMayvo6"
      },
      "source": [
        "## Grid Search CV - Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fizLJ_YQ6elb"
      },
      "source": [
        "### Use standard hyperparameters to find most suitable algorithm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kMgswohfKBda"
      },
      "outputs": [],
      "source": [
        "models_quick_search = {\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=0),\n",
        "    \"XGBClassifier\": XGBClassifier(random_state=0),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=0),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(random_state=0),\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=0),\n",
        "    \"ExtraTreesClassifier\": ExtraTreesClassifier(random_state=0),\n",
        "    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=0),\n",
        "}\n",
        "\n",
        "params_quick_search = {\n",
        "    \"LogisticRegression\": {},\n",
        "    \"XGBClassifier\": {},\n",
        "    \"DecisionTreeClassifier\": {},\n",
        "    \"RandomForestClassifier\": {},\n",
        "    \"GradientBoostingClassifier\": {},\n",
        "    \"ExtraTreesClassifier\": {},\n",
        "    \"AdaBoostClassifier\": {},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXu0Ryeown7N"
      },
      "source": [
        "Quick GridSearch CV - Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7eLJcKEKBlQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, recall_score\n",
        "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0bkL-IxwnJx"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpFOc7OAKMuz"
      },
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewezVDt46jTJ"
      },
      "source": [
        "### Do an extensive search on the most suitable algorithm to find the best hyperparameter configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1WozH5frBQ9"
      },
      "source": [
        "Define model and parameters, for Extensive Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sDT_WMUErBRB"
      },
      "outputs": [],
      "source": [
        "models_search = {\n",
        "    \"XGBClassifier\":XGBClassifier(random_state=0),\n",
        "}\n",
        "\n",
        "# documentation to help on hyperparameter list: \n",
        "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
        "\n",
        "# We will not conduct an extensive search, since the focus\n",
        "# is on how to combine all knowledge in an applied project.\n",
        "# In a workplace project, you may spend more time in this step\n",
        "params_search = {\n",
        "    \"XGBClassifier\":{\n",
        "        'model__learning_rate': [1e-1,1e-2,1e-3], \n",
        "        'model__max_depth': [3,10,None],\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP2Ua0FGrBRC"
      },
      "source": [
        "Extensive GridSearch CV - Binary Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK1s893orBRD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score, make_scorer\n",
        "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8oVKtHyr-X8"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AFyZ6-pr9tN"
      },
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get best model name programmatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htAXEVFpwiBV"
      },
      "source": [
        "Parameters for best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDIt27RdKOG8"
      },
      "outputs": [],
      "source": [
        "best_parameters = grid_search_pipelines[best_model].best_params_\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAnJQlDlw1FE"
      },
      "source": [
        "Define the best clf pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLotNfy4MKDE"
      },
      "outputs": [],
      "source": [
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgdxKijH6qJS"
      },
      "source": [
        "## Assess feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n30pl2dowzW3"
      },
      "source": [
        "* With the current model, we can assess with `.features_importances_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XGczhv2uo2C"
      },
      "outputs": [],
      "source": [
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': X_train.columns[pipeline_clf['feat_selection'].get_support()],\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# re-assign best_features order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "## Evaluate Pipeline on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myG6tDSGan4r"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
        "\n",
        "    prediction = pipeline.predict(X)\n",
        "\n",
        "    print('---  Confusion Matrix  ---')\n",
        "    print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
        "          columns=[[\"Actual \" + sub for sub in label_map]],\n",
        "          index=[[\"Prediction \" + sub for sub in label_map]]\n",
        "          ))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print('---  Classification Report  ---')\n",
        "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
        "\n",
        "\n",
        "def clf_performance(X_train, y_train, X_test, y_test, pipeline, label_map):\n",
        "    print(\"#### Train Set #### \\n\")\n",
        "    confusion_matrix_and_report(X_train, y_train, pipeline, label_map)\n",
        "\n",
        "    print(\"#### Test Set ####\\n\")\n",
        "    confusion_matrix_and_report(X_test, y_test, pipeline, label_map)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpUfEAGlW5aK"
      },
      "source": [
        "Evaluation: We cross check with metrics defined at ML business case\n",
        "* 80% Recall for Churn, on train and test set\n",
        "* 80% Precision for no Churn on train and test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umWjIvGMNLig"
      },
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_clf,\n",
        "                label_map= ['No Churn', 'Churn'] \n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WgttWjtHHOQ"
      },
      "source": [
        "# Step 3: Refit pipeline with best features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCyOyebVHVmA"
      },
      "source": [
        "## Refit ML Pipeline and Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4PpI2sKC5IL"
      },
      "source": [
        "In theory, a pipeline fitted **using only the most important features** should give the same result as the one fitted with **all variables and feature selection**\n",
        "\n",
        "* However, in this project we have a step for feature augmentation, which is to balance the target Train Set using SMOTE().\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km_-hW0f68DP"
      },
      "source": [
        "## Rewrite ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBeckIjkCa4k"
      },
      "source": [
        "New Pipeline for DataCleaning And FeatureEngineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc8ptvFiHJmb"
      },
      "outputs": [],
      "source": [
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "    pipeline_base = Pipeline([\n",
        "\n",
        "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['InternetService', 'Contract'])),\n",
        "\n",
        "\n",
        "        # we don't need SmartCorrelatedSelection\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGNs9PU16_Ls"
      },
      "source": [
        "## Rewrite ML Pipeline for Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpjmxzTbCXlg"
      },
      "source": [
        "Function for Pipeline optmisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E76QmoMEWA0"
      },
      "outputs": [],
      "source": [
        "# Pipeline Optmization: Model\n",
        "def PipelineClf(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        # no feature selection needed anymore!!! We know which features to use already!\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75hfh3o5GhoU"
      },
      "source": [
        "## Split Train Test Set, considering only with best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6dX0VeKGhod"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(['Churn'], axis=1),\n",
        "    df['Churn'],\n",
        "    test_size=0.2,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19a3t6jI6H6"
      },
      "source": [
        "We filter only the most important variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5Acb9T_GXjU"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.filter(best_features)\n",
        "X_test = X_test.filter(best_features)\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjOcRGheGhof"
      },
      "source": [
        "## Handle Target Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbQda_pcGhof"
      },
      "outputs": [],
      "source": [
        "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
        "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train)\n",
        "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxIFw3KGhog"
      },
      "source": [
        "Check Train Set Target distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZQyth-2Ghog"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "y_train.value_counts().plot(kind='bar', title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9FbCbIrGhoh"
      },
      "source": [
        "Use SMOTE to balance Train Set target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtbWft5VGhoh"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "oversample = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YozwzI9eGhoh"
      },
      "source": [
        "Check Train Set Target distribution after SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyL99cYMGhoi"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts().plot(kind='bar',title='Train Set Target Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_WjvD_QIJ_L"
      },
      "source": [
        "## Grid Search CV: Sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESkqrySI7N6u"
      },
      "source": [
        "Using the most suitable model from the last section and its best hyperparameter configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fFaXDOIJ_M"
      },
      "source": [
        "We are using the same model from  the last GridCV search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7F0z__h1qSA"
      },
      "outputs": [],
      "source": [
        "models_search   # XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRteBPgd3ldU"
      },
      "source": [
        "And the best parameters from the last GridCV search "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbGNBeZk3V8r"
      },
      "outputs": [],
      "source": [
        "best_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlLJP5Ds3rYp"
      },
      "source": [
        "You will need to type in manually since the hyperparameter values have to be a list. The previous dictionary is not in this format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bC8RmE-2Mi2"
      },
      "outputs": [],
      "source": [
        "params_search = {'XGBClassifier':  {\n",
        "    'model__learning_rate': [0.01],   # the value should be in []\n",
        "    'model__max_depth': [3]},  # the value should be in []\n",
        "}\n",
        "params_search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrRJNywsIJ_M"
      },
      "source": [
        "GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv5nO6cJP9fX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score, make_scorer\n",
        "quick_search = HyperparameterOptimizationSearch(\n",
        "    models=models_search, params=params_search)\n",
        "quick_search.fit(X_train, y_train,\n",
        "                 scoring=make_scorer(recall_score, pos_label=1),\n",
        "                 n_jobs=-1, cv=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_Yu9ykIJ_N"
      },
      "source": [
        "Check results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqIk1g95IJ_N"
      },
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZcP3yXpIJ_O"
      },
      "source": [
        "Define the best clf pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7Qe2jFEIJ_O"
      },
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0, 0]\n",
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEXWvsb7Su0"
      },
      "source": [
        "## Assess feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8UGZ5bnIJ_P"
      },
      "outputs": [],
      "source": [
        "best_features = X_train.columns\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': best_features,\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQF20xan7VuK"
      },
      "source": [
        "## Evaluate Pipeline on Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation: We cross-check with metrics defined in the ML business case.\n",
        "* 80% Recall for Churn, on train and test set.\n",
        "* 80% Precision for no Churn on train and test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cpCj2lLHxB-"
      },
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_clf,\n",
        "                label_map= ['No Churn', 'Churn'] \n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBVunRgBqIXQ"
      },
      "source": [
        "# Step 4: Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxnlKI5SJcoO"
      },
      "source": [
        "We will generate the following files\n",
        "* Train set\n",
        "* Test set\n",
        "* Data cleaning and Feature Engineering pipeline\n",
        "* Modeling pipeline\n",
        "* features importance plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16bIOgs3J7OD"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/predict_churn/{version}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e-gC6sa7hpj"
      },
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHZUZKJ5JiKn"
      },
      "source": [
        "* note that the variables **are transformed already** in X_train and the shape is 8266 - after SMOTE was applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc4fzrdTJno1"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qzq7DgVTJnv3"
      },
      "outputs": [],
      "source": [
        "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzPsdNGX9gtf"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMoT1cJ39g26"
      },
      "outputs": [],
      "source": [
        "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYatlgsj7pbB"
      },
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEKp3-dJJn3p"
      },
      "source": [
        "* note that the variables are transformed already in X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UMg2vPtJqxM"
      },
      "outputs": [],
      "source": [
        "print(X_test.shape)\n",
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz2OqPW6Jqzv"
      },
      "outputs": [],
      "source": [
        "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pPTVz219xj3"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap7fYYAm9xsj"
      },
      "outputs": [],
      "source": [
        "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ufHAplN7tdo"
      },
      "source": [
        "## ML Pipelines: Data Cleaning and Feat Eng pipeline and Modelling Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAbbAO2r248W"
      },
      "source": [
        "We will save 2 pipelines: \n",
        "* Both should be used in conjunction to predict Live Data.\n",
        "* To predict on Train Set, Test Set we use only pipeline_clf, since the data is already processed.\n",
        "\n",
        "\n",
        "\n",
        "Pipeline responsible for Data Cleaning and Feature Engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCcAlvoG3CRm"
      },
      "outputs": [],
      "source": [
        "pipeline_data_cleaning_feat_eng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaHdCf4HKBLg"
      },
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_data_cleaning_feat_eng ,\n",
        "            filename=f\"{file_path}/clf_pipeline_data_cleaning_feat_eng.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE-iU6TL3LVI"
      },
      "source": [
        "* Pipeline responsible for Feature Scaling, and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zEBxfvBqI29"
      },
      "outputs": [],
      "source": [
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObL5Iz8tKdsZ"
      },
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_clf ,\n",
        "            filename=f\"{file_path}/clf_pipeline_model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqEUyLG27v9N"
      },
      "source": [
        "## Feature Importance plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBiqB55L1Qhk"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR0taWpn1RuD"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
        "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Good job, you should clear outputs, then run git commands to push files to the repo. Next, move on to Predict Tenure notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modeling and Evaluation - Predict Customer Churn.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
